"""
Template Generator
Generate training scripts, templates, and documentation
"""

from pathlib import Path
from typing import Dict, Any
from datetime import datetime

from src.helpers import Logger


class TemplateGenerator:
    """Generate templates and documentation"""
    
    def __init__(self, config, paths, logger: Logger):
        self.config = config
        self.paths = paths
        self.logger = logger
    
    def generate_all_templates(self):
        """Generate all training templates"""
        self.logger.info("Generating training templates...")
        
        templates_dir = self.paths.get('templates_dir')
        templates_dir.mkdir(exist_ok=True)
        
        # Generate LoRA training template
        self._generate_lora_template()
        
        # Generate QLoRA training template
        self._generate_qlora_template()
        
        # Generate inference template
        self._generate_inference_template()
        
        # Generate activation script
        self._generate_activation_script()
        
        self.logger.success("✓ Templates generated")
    
    def _generate_lora_template(self):
        """Generate LoRA training template"""
        template = '''#!/usr/bin/env python3
"""
LoRA Fine-tuning Template
Generated by AI Framework Setup
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer
from datasets import load_dataset

def main():
    # Configuration
    model_name = "meta-llama/Llama-2-7b-hf"  # Change as needed
    dataset_name = "your-dataset"
    output_dir = "./lora-output"
    
    # LoRA configuration
    lora_config = LoraConfig(
        r=16,  # Rank
        lora_alpha=32,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    # Load model and tokenizer
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # Apply LoRA
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # Load dataset
    dataset = load_dataset(dataset_name, split="train")
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        num_train_epochs=3,
        logging_steps=10,
        save_steps=100,
        fp16=True,
    )
    
    # Trainer
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        dataset_text_field="text",
        max_seq_length=512,
    )
    
    # Train
    trainer.train()
    
    # Save
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)

if __name__ == "__main__":
    main()
'''
        
        template_path = self.paths.get('templates_dir') / 'lora_training.py'
        with open(template_path, 'w') as f:
            f.write(template)
        template_path.chmod(0o755)
    
    def _generate_qlora_template(self):
        """Generate QLoRA training template"""
        template = '''#!/usr/bin/env python3
"""
QLoRA Fine-tuning Template (4-bit quantization)
Generated by AI Framework Setup
"""

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer
from datasets import load_dataset

def main():
    # Configuration
    model_name = "meta-llama/Llama-2-7b-hf"
    dataset_name = "your-dataset"
    output_dir = "./qlora-output"
    
    # QLoRA: 4-bit quantization config
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )
    
    # LoRA configuration
    lora_config = LoraConfig(
        r=64,  # Higher rank for QLoRA
        lora_alpha=128,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        lora_dropout=0.1,
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    # Load model with quantization
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True
    )
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    
    # Prepare model for k-bit training
    model = prepare_model_for_kbit_training(model)
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    
    # Load dataset
    dataset = load_dataset(dataset_name, split="train")
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=8,
        learning_rate=2e-4,
        num_train_epochs=3,
        logging_steps=10,
        save_steps=100,
        bf16=True,  # Use bfloat16 for QLoRA
        optim="paged_adamw_8bit",
    )
    
    # Trainer
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        dataset_text_field="text",
        max_seq_length=1024,
    )
    
    # Train
    trainer.train()
    
    # Save
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)

if __name__ == "__main__":
    main()
'''
        
        template_path = self.paths.get('templates_dir') / 'qlora_training.py'
        with open(template_path, 'w') as f:
            f.write(template)
        template_path.chmod(0o755)
    
    def _generate_inference_template(self):
        """Generate inference template"""
        template = '''#!/usr/bin/env python3
"""
Inference Template
Generated by AI Framework Setup
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

def load_model(base_model_path, adapter_path=None):
    """Load model with optional LoRA adapter"""
    tokenizer = AutoTokenizer.from_pretrained(base_model_path)
    model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    if adapter_path:
        model = PeftModel.from_pretrained(model, adapter_path)
        model = model.merge_and_unload()
    
    return model, tokenizer

def generate_text(model, tokenizer, prompt, max_length=512):
    """Generate text from prompt"""
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
        )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def main():
    # Configuration
    base_model = "meta-llama/Llama-2-7b-hf"
    adapter_path = None  # Set to your LoRA adapter path if needed
    
    # Load model
    print("Loading model...")
    model, tokenizer = load_model(base_model, adapter_path)
    print("Model loaded!")
    
    # Interactive loop
    while True:
        prompt = input("\\nPrompt (or 'quit'): ")
        if prompt.lower() == 'quit':
            break
        
        response = generate_text(model, tokenizer, prompt)
        print(f"\\nResponse:\\n{response}")

if __name__ == "__main__":
    main()
'''
        
        template_path = self.paths.get('templates_dir') / 'inference.py'
        with open(template_path, 'w') as f:
            f.write(template)
        template_path.chmod(0o755)
    
    def _generate_activation_script(self):
        """Generate environment activation script"""
        script = f'''#!/bin/zsh
# AI Framework Environment Activation
# Generated: {datetime.now().isoformat()}

VENV_ROOT="{self.config.VENV_ROOT}"

activate_env() {{
    local env_name=$1
    local venv_path="$VENV_ROOT/$env_name"
    
    if [[ ! -d "$venv_path" ]]; then
        echo "Error: Environment '$env_name' not found"
        return 1
    fi
    
    source "$venv_path/bin/activate"
    echo "✓ Activated: $env_name"
}}

# Available environments
'''
        
        for venv_name, description in self.config.VENVS.items():
            env_short = venv_name.replace('pytorch-', '')
            script += f'alias activate-{env_short}="activate_env {venv_name}"\n'
        
        script += '''
echo "Available environments:"
'''
        for venv_name, description in self.config.VENVS.items():
            env_short = venv_name.replace('pytorch-', '')
            script += f'echo "  activate-{env_short:<12} - {description}"\n'
        
        activate_path = self.paths.get('framework_root') / 'activate.zsh'
        with open(activate_path, 'w') as f:
            f.write(script)
        activate_path.chmod(0o755)
    
    def generate_commands_md(self):
        """Generate COMMANDS.md documentation"""
        content = f'''# AI Framework Commands

Generated: {datetime.now().isoformat()}

## Quick Start

```bash
# Activate the framework environment
source {self.config.FRAMEWORK_ROOT}/activate.zsh

# Start training
activate-training
cd {self.config.TEMPLATES_DIR}
python3 lora_training.py

# Start Jupyter Lab
activate-training
jupyter lab --port {self.config.JUPYTER_PORT}
```

## Available Environments

'''
        
        for venv_name, description in self.config.VENVS.items():
            env_short = venv_name.replace('pytorch-', '')
            content += f'''### {venv_name}
**Description:** {description}  
**Activate:** `activate-{env_short}`  
**Path:** `{self.config.VENV_ROOT}/{venv_name}`

'''
        
        content += f'''## Training Templates

All templates are in: `{self.config.TEMPLATES_DIR}/`

### LoRA Fine-tuning
```bash
activate-training
python3 {self.config.TEMPLATES_DIR}/lora_training.py
```

### QLoRA Fine-tuning (4-bit)
```bash
activate-tuning
python3 {self.config.TEMPLATES_DIR}/qlora_training.py
```

### Inference
```bash
activate-runtime
python3 {self.config.TEMPLATES_DIR}/inference.py
```

## Jupyter Lab

### Start Jupyter
```bash
activate-training
jupyter lab --port {self.config.JUPYTER_PORT} --no-browser
```

### Install kernel in other environments
```bash
activate-experiments
python -m ipykernel install --user --name=pytorch-experiments
```

## GPU Monitoring

```bash
# Watch GPU usage
watch -n 1 nvidia-smi

# GPU memory details
nvidia-smi --query-gpu=memory.used,memory.total --format=csv
```

## Common Tasks

### Download a model
```bash
activate-training
python3 -c "from transformers import AutoModel; AutoModel.from_pretrained('meta-llama/Llama-2-7b-hf')"
```

### Check installed packages
```bash
activate-training
pip list | grep -E "(torch|transformers|peft)"
```

### Update packages
```bash
activate-training
pip install --upgrade transformers accelerate peft
```

## Memory Management

### Clear CUDA cache
```python
import torch
torch.cuda.empty_cache()
```

### Monitor system memory
```bash
free -h
cat /proc/meminfo | grep -E "(MemTotal|MemAvailable)"
```

## Troubleshooting

### Out of Memory (OOM)
- Reduce batch size
- Enable gradient checkpointing
- Use gradient accumulation
- Try QLoRA instead of LoRA

### CUDA not available
```bash
python3 -c "import torch; print(torch.cuda.is_available())"
nvidia-smi
```

### Package conflicts
```bash
activate-training
pip install --force-reinstall torch transformers
```

## Configuration Files

- **Framework Config:** `{self.config.FRAMEWORK_ROOT}/.frameworkrc`
- **CUDA Environment:** `~/.bashrc` (CUDA_HOME section)
- **System Memory:** `/etc/sysctl.d/99-training-memory.conf`
- **Jupyter Config:** `~/.jupyter/jupyter_lab_config.py`

## Useful Links

- PyTorch Docs: https://pytorch.org/docs/
- Transformers Docs: https://huggingface.co/docs/transformers
- PEFT Docs: https://huggingface.co/docs/peft
- TRL Docs: https://huggingface.co/docs/trl
'''
        
        commands_path = self.paths.get('docs_dir') / 'COMMANDS.md'
        with open(commands_path, 'w') as f:
            f.write(content)
        
        self.logger.success(f"✓ Generated {commands_path}")
    
    def generate_architecture_md(self):
        """Generate ARCHITECTURE.md documentation"""
        content = f'''# Framework Architecture

Generated: {datetime.now().isoformat()}

## Directory Structure

```
{self.config.FRAMEWORK_ROOT}/
├── main.py                      # Entry point
├── .frameworkrc                 # Configuration overrides (YAML)
├── activate.zsh                 # Environment activation script
│
├── core/
│   ├── config.py               # Core configuration (EDIT THIS)
│   ├── paths.py                # Path management (EDIT THIS)
│   ├── types.py                # Type definitions
│   ├── packages.py             # Package definitions (EDIT THIS)
│   └── logic.py                # Orchestration logic
│
├── src/
│   ├── helpers.py              # Logging, colors, formatting
│   ├── handlers.py             # Error handling, recovery
│   ├── utils.py                # System operations
│   ├── validators.py           # Pre-flight checks
│   └── generate.py             # Template generation
│
├── templates/
│   ├── lora_training.py
│   ├── qlora_training.py
│   └── inference.py
│
├── docs/
│   ├── COMMANDS.md             # User commands
│   └── ARCHITECTURE.md         # This file
│
└── logs/
    ├── setup-TIMESTAMP.log     # Setup logs
    └── .last_checkpoint        # Resume checkpoint
```

## Making Changes

### To change infrastructure settings:
1. Edit `core/config.py` - Change versions, paths, flags
2. Edit `core/paths.py` - Change directory structure
3. Edit `core/packages.py` - Add/remove packages

### To extend functionality:
1. Add functions to `src/utils.py` - System operations
2. Add validators to `src/validators.py` - Pre-flight checks
3. Add templates to `src/generate.py` - New training scripts

### Configuration Priority:
1. `.frameworkrc` (highest - user overrides)
2. `core/config.py` (defaults)
3. Environment variables (lowest)

## Execution Flow

```
main.py
  ↓
Load core/config.py
  ↓
Load core/paths.py
  ↓
Source all src/*.py modules
  ↓
core/logic.py (orchestrate)
  ├→ src/validators.py (check system)
  ├→ src/utils.py (execute operations)
  ├→ src/handlers.py (handle errors)
  └→ src/generate.py (create templates)
```

## Key Design Principles

1. **Separation of Concerns**
   - `core/` = What to do (configuration)
   - `src/` = How to do it (implementation)

2. **Modularity**
   - Each file has single responsibility
   - Functions are reusable
   - Easy to test independently

3. **Error Handling**
   - Checkpoint system for resume
   - Graceful failure recovery
   - Detailed logging

4. **Extensibility**
   - Add new environments: Edit `config.py::VENVS`
   - Add new packages: Edit `packages.py`
   - Add new templates: Edit `generate.py`

## Adding a New Environment

1. Edit `core/config.py`:
```python
self.VENVS = {{
    "pytorch-training": "Training with LoRA/QLoRA",
    "pytorch-custom": "My custom environment",  # ADD THIS
}}
```

2. Run setup:
```bash
python3 main.py
```

3. New environment created at `{self.config.VENV_ROOT}/pytorch-custom`

## Adding New Packages

1. Edit `core/packages.py`:
```python
"my-package": Package(
    name="my-package",
    version="1.0.0",
    description="My custom package"
),
```

2. Re-run package installation:
```bash
python3 main.py --force
```

## Resume from Checkpoint

If setup fails or is interrupted:

```bash
python3 main.py --resume
```

Checkpoint data stored in: `{self.config.FRAMEWORK_ROOT}/logs/.last_checkpoint`

## Dry Run Mode

Test what would happen without making changes:

```bash
python3 main.py --dry-run
```

## Logging

- Console output: Colored, real-time
- File output: `logs/setup-TIMESTAMP.log`
- Verbose mode: `python3 main.py --verbose`

## Support

For issues or questions:
1. Check `docs/COMMANDS.md`
2. Check logs in `logs/`
3. Review last checkpoint
4. Re-run with `--verbose`
'''
        
        arch_path = self.paths.get('docs_dir') / 'ARCHITECTURE.md'
        with open(arch_path, 'w') as f:
            f.write(content)
        
        self.logger.success(f"✓ Generated {arch_path}")