| Use Case            | What You Can Build                                                     |
| ------------------- | ---------------------------------------------------------------------- |
| ğŸ” Security         | Encrypt prompt history, gated access to model                          |
| ğŸ® Games            | NPCs that respond using a local model                                  |
| ğŸ§° Tools            | Local agent CLI (e.g. dev assistant, planning tool)                    |
| ğŸ“Ÿ APIs             | Wrap it into a JSON HTTP server or WebSocket                           |
| ğŸ“± Mobile           | Embed into a Rust-powered mobile app with Tauri or Rust-native wrapper |
| ğŸªµ Logging          | Intercept tokens, trace memory, log decisions for analysis             |
| ğŸ§  RL / Fine-tuning | Modify sampling, apply reinforcement logic, auto-labeling tools        |

| Goal                   | Tooling                                                   |
| ---------------------- | --------------------------------------------------------- |
| ğŸ§  Add multi-turn chat | Track KV cache + token history                            |
| ğŸ§µ Add streaming       | Use `llama_decode` in a loop, print tokens one-by-one     |
| ğŸŒ Build an API        | Wrap it in `axum`, `warp`, or `actix-web`                 |
| ğŸ› Add control knobs   | Token limits, temperature, top-p, frequency penalty       |
| ğŸ” Lock it             | Use `ring` or `aes-gcm` to encrypt unlock keys            |
| ğŸ§ª Add tests           | Use `proptest` or `quickcheck` to fuzz prompt input       |
| ğŸ–¼ Serve a UI          | Hook into `tauri`, `leptos`, or `yew` for local UI        |
| âš¡ Speed tune          | Profile with `perf`, `flamegraph`, or `cargo instruments` |

| Capability                  | What It Means                                        |
| --------------------------- | ---------------------------------------------------- |
| ğŸ§  `llama_tokenize()`       | Convert raw strings to tokens with full vocab access |
| âš™ï¸ `llama_decode()`         | Feed batches to the model, run inference manually    |
| ğŸ§µ `llama_kv_*`             | Work with memory state (KV cache) per session        |
| ğŸ” `llama_sampler_*`        | Control temperature, repetition penalty, top-p, etc. |
| ğŸ“¤ `llama_token_to_piece()` | Convert model outputs back to text                   |
| ğŸ—‚ Load multiple models     | Swap GGUF files on the fly in Rust logic             |
| ğŸ” Bind it to system        | Lock model behind AES, tokens, license, etc.         |
| ğŸš€ Fully offline              | No Python, no servers, no PyTorch or CUDA required   |
